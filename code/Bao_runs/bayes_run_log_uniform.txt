Read 40 queries.
Using Bao: True
Executing queries using PG optimizer for initial training
x x 1747321603.9192452 sample_queries/q11_17e.sql 8.030191898345947 PG
x x 1747321611.7104921 sample_queries/q35_1a1508.sql 7.791187763214111 PG
x x 1747321615.7627246 sample_queries/q15_18a.sql 4.052198648452759 PG
x x 1747321616.8231783 sample_queries/q27_3c.sql 1.060417890548706 PG
x x 1747321619.474163 sample_queries/q34_1a275.sql 2.6509506702423096 PG
x x 1747321627.485166 sample_queries/q39_2a2781.sql 8.010966062545776 PG
x x 1747321627.7464402 sample_queries/q27_3c.sql 0.26124143600463867 PG
x x 1747321628.6264305 sample_queries/q28_13a.sql 0.8799474239349365 PG
x x 1747321632.3182466 sample_queries/q10_2a265.sql 3.6917800903320312 PG
x x 1747321636.4436085 sample_queries/q26_2a274.sql 4.125328063964844 PG
x x 1747321645.0467482 sample_queries/q23_19d.sql 8.603106498718262 PG
x x 1747321649.8202562 sample_queries/q4_8a122.sql 4.773475170135498 PG
x x 1747321688.6739507 sample_queries/q2_8a82.sql 38.85366201400757 PG
x x 1747321697.436386 sample_queries/q38_2a1870.sql 8.762394666671753 PG
x x 1747321700.459317 sample_queries/q26_2a274.sql 3.0228896141052246 PG
x x 1747321702.255232 sample_queries/q17_7a164.sql 1.7958812713623047 PG
x x 1747321703.2932847 sample_queries/q4_8a122.sql 1.038017749786377 PG
x x 1747321705.011021 sample_queries/q3_7a99.sql 1.7176933288574219 PG
x x 1747321705.0561192 sample_queries/q24_32a.sql 0.04505658149719238 PG
x x 1747321705.972212 sample_queries/q13_7a121.sql 0.9160566329956055 PG
x x 1747321706.0276003 sample_queries/q29_6e.sql 0.05535459518432617 PG
x x 1747321706.645372 sample_queries/q28_13a.sql 0.6177306175231934 PG
x x 1747321716.6337037 sample_queries/q38_2a1870.sql 9.988298892974854 PG
x x 1747321717.1493838 sample_queries/q31_2a39.sql 0.5156490802764893 PG
x x 1747321717.2037938 sample_queries/q29_6e.sql 0.05437517166137695 PG
Initial input channels: 10
Epoch 0 training loss: 2.288632571697235
Epoch 15 training loss: 0.2734871134161949
Epoch 30 training loss: 0.45154690742492676
Epoch 45 training loss: 0.7679625898599625
Epoch 60 training loss: 0.525055930018425
Epoch 75 training loss: 0.6197440326213837
Epoch 90 training loss: 0.42415425181388855
Epoch 105 training loss: 0.46835649013519287
Epoch 120 training loss: 0.3102496713399887
Epoch 135 training loss: 0.27518320828676224
Epoch 150 training loss: 0.43471935391426086
Epoch 165 training loss: 0.39728400111198425
Epoch 180 training loss: 0.43607297539711
Epoch 195 training loss: 0.41210591048002243
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
0 0 1747321722.2064974 sample_queries/q31_2a39.sql 0.5561213493347168
0 1 1747321722.2536235 sample_queries/q24_32a.sql 0.0470883846282959
0 2 1747321727.454091 sample_queries/q11_17e.sql 5.2004311084747314
0 3 1747321732.11235 sample_queries/q12_17a.sql 4.658217430114746
0 4 1747321733.486183 sample_queries/q7_7a48.sql 1.3737988471984863
0 5 1747321746.355393 sample_queries/q40_2a8120.sql 12.86917519569397
0 6 1747321746.5181856 sample_queries/q20_24b.sql 0.16276073455810547
0 7 1747321755.0870683 sample_queries/q5_8a423.sql 8.568848609924316
0 8 1747321755.145916 sample_queries/q29_6e.sql 0.058814048767089844
0 9 1747321759.216569 sample_queries/q14_6a349.sql 4.070613622665405
0 10 1747321760.3726628 sample_queries/q4_8a122.sql 1.1560494899749756
0 11 1747321760.9760745 sample_queries/q31_2a39.sql 0.6033768653869629
0 12 1747321762.546336 sample_queries/q7_7a48.sql 1.5702266693115234
0 13 1747321764.3577795 sample_queries/q35_1a1508.sql 1.8114078044891357
0 14 1747321770.6419227 sample_queries/q13_7a121.sql 6.284109115600586
0 15 1747321770.9017515 sample_queries/q32_2a493.sql 0.2597925662994385
0 16 1747321783.0522308 sample_queries/q39_2a2781.sql 12.15044093132019
0 17 1747321791.5452788 sample_queries/q5_8a423.sql 8.49301266670227
0 18 1747321820.2381923 sample_queries/q22_8a27.sql 28.692879676818848
0 19 1747321856.6814787 sample_queries/q2_8a82.sql 36.44324254989624
0 20 1747321859.9103665 sample_queries/q36_7a136.sql 3.228849172592163
0 21 1747321862.372575 sample_queries/q16_26c.sql 2.462172269821167
0 22 1747321873.6022408 sample_queries/q40_2a8120.sql 11.229631185531616
0 23 1747321885.2131586 sample_queries/q39_2a2781.sql 11.610875844955444
0 24 1747321890.1217086 sample_queries/q12_17a.sql 4.908507585525513
Initial input channels: 10
Epoch 0 training loss: 0.2889475002884865
Epoch 15 training loss: 0.3086554358014837
Epoch 30 training loss: 0.6488588377833366
Epoch 45 training loss: 0.6793744564056396
Epoch 60 training loss: 0.359467588365078
Epoch 75 training loss: 0.7448977082967758
Epoch 90 training loss: 0.3498285450041294
Epoch 105 training loss: 0.6909328326582909
Epoch 120 training loss: 0.6772670987993479
Epoch 135 training loss: 0.4896939378231764
Epoch 150 training loss: 0.6328984722495079
Epoch 165 training loss: 0.3465785695007071
Epoch 180 training loss: 0.4926491007208824
Epoch 195 training loss: 0.4454416446387768
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
1 0 1747321900.4341648 sample_queries/q31_2a39.sql 4.6955482959747314
1 1 1747321902.807598 sample_queries/q23_19d.sql 2.3733973503112793
1 2 1747321905.3550947 sample_queries/q17_7a164.sql 2.5474603176116943
1 3 1747321905.5945902 sample_queries/q32_2a493.sql 0.23945999145507812
1 4 1747321915.5323398 sample_queries/q8_6a505.sql 9.937716245651245
1 5 1747321917.515447 sample_queries/q30_18c.sql 1.9830586910247803
1 6 1747321920.2310157 sample_queries/q14_6a349.sql 2.7155261039733887
1 7 1747321920.8501923 sample_queries/q28_13a.sql 0.6191356182098389
1 8 1747321929.1717093 sample_queries/q39_2a2781.sql 8.32148027420044
1 9 1747321934.8121734 sample_queries/q8_6a505.sql 5.640415906906128
1 10 1747321938.9911997 sample_queries/q36_7a136.sql 4.178988695144653
1 11 1747321949.9236753 sample_queries/q11_17e.sql 10.932438373565674
1 12 1747321955.8185787 sample_queries/q8_6a505.sql 5.894863843917847
1 13 1747321956.0653973 sample_queries/q32_2a493.sql 0.24677586555480957
1 14 1747321957.5106244 sample_queries/q13_7a121.sql 1.4451947212219238
1 15 1747321959.1879532 sample_queries/q15_18a.sql 1.677290439605713
1 16 1747321960.8034332 sample_queries/q13_7a121.sql 1.6154448986053467
1 17 1747321972.8742146 sample_queries/q34_1a275.sql 12.070741415023804
1 18 1747321974.0069714 sample_queries/q16_26c.sql 1.1327135562896729
1 19 1747322035.7628477 sample_queries/q3_7a99.sql 61.75583457946777
1 20 1747322038.6224682 sample_queries/q14_6a349.sql 2.859576463699341
1 21 1747322118.7675312 sample_queries/q2_8a82.sql 80.14502596855164
1 22 1747322121.6131792 sample_queries/q26_2a274.sql 2.8456125259399414
1 23 1747322126.5636017 sample_queries/q11_17e.sql 4.9503889083862305
1 24 1747322127.934239 sample_queries/q10_2a265.sql 1.3705902099609375
Initial input channels: 10
Epoch 0 training loss: 2.117533195018768
Epoch 15 training loss: 0.3079368323087692
Epoch 30 training loss: 0.4740180969238281
Epoch 45 training loss: 0.46999220848083495
Epoch 60 training loss: 0.3639729082584381
Epoch 75 training loss: 0.4389965653419495
Epoch 90 training loss: 0.3819650262594223
Epoch 105 training loss: 0.33334723860025406
Epoch 120 training loss: 0.35237847864627836
Epoch 135 training loss: 0.40383286476135255
Epoch 150 training loss: 0.3646850407123566
Epoch 165 training loss: 0.4132005929946899
Epoch 180 training loss: 0.3905960738658905
Epoch 195 training loss: 0.32069502472877504
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
2 0 1747322147.1123786 sample_queries/q6_16b.sql 11.57737112045288
2 1 1747322151.7950032 sample_queries/q9_5a48.sql 4.682591676712036
2 2 1747322154.5473135 sample_queries/q14_6a349.sql 2.75227689743042
2 3 1747322160.802321 sample_queries/q37_2a1291.sql 6.254974126815796
2 4 1747322166.8624783 sample_queries/q9_5a48.sql 6.060122728347778
2 5 1747322179.000249 sample_queries/q6_16b.sql 12.137734413146973
2 6 1747322185.0408823 sample_queries/q5_8a423.sql 6.040601015090942
2 7 1747322213.77579 sample_queries/q22_8a27.sql 28.73487377166748
2 8 1747322216.8758373 sample_queries/q36_7a136.sql 3.1000053882598877
2 9 1747322217.5130267 sample_queries/q28_13a.sql 0.6371541023254395
2 10 1747322232.1391397 sample_queries/q7_7a48.sql 14.626070499420166
2 11 1747322234.9975536 sample_queries/q36_7a136.sql 2.858366012573242
2 12 1747322241.3943162 sample_queries/q40_2a8120.sql 6.396728038787842
2 13 1747322245.73186 sample_queries/q4_8a122.sql 4.337506294250488
2 14 1747322245.869041 sample_queries/q24_32a.sql 0.1371479034423828
2 15 1747322248.8284826 sample_queries/q36_7a136.sql 2.959407091140747
2 16 1747322249.7764852 sample_queries/q13_7a121.sql 0.9479575157165527
2 17 1747322253.6972024 sample_queries/q18_7a103.sql 3.920682191848755
2 18 1747322267.9225464 sample_queries/q7_7a48.sql 14.225306749343872
2 19 1747322268.789407 sample_queries/q25_13d.sql 0.8668239116668701
2 20 1747322270.400031 sample_queries/q35_1a1508.sql 1.6105904579162598
2 21 1747322270.9342048 sample_queries/q31_2a39.sql 0.5341370105743408
2 22 1747322279.7411277 sample_queries/q1_8a463.sql 8.80688762664795
2 23 1747322280.2682424 sample_queries/q31_2a39.sql 0.5270805358886719
2 24 1747322288.6099248 sample_queries/q6_16b.sql 8.341647863388062
Initial input channels: 10
Epoch 0 training loss: 3.247138504471098
Epoch 15 training loss: 0.6489579081535339
Epoch 30 training loss: 0.35333025242601124
Epoch 45 training loss: 0.5158689554248538
Epoch 60 training loss: 0.4706241190433502
Epoch 75 training loss: 0.5402764550277165
Epoch 90 training loss: 0.4612186976841518
Epoch 105 training loss: 0.35981750914028715
Epoch 120 training loss: 0.3840304123503821
Epoch 135 training loss: 0.44853417362485615
Epoch 150 training loss: 0.33142235449382235
Epoch 165 training loss: 0.5487632432154247
Epoch 180 training loss: 0.38994933451925007
Epoch 195 training loss: 0.3943261610610144
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
3 0 1747322302.487691 sample_queries/q22_8a27.sql 4.532574415206909
3 1 1747322306.7932644 sample_queries/q22_8a27.sql 4.305537700653076
3 2 1747322312.8920412 sample_queries/q17_7a164.sql 6.098735809326172
3 3 1747322315.9000328 sample_queries/q17_7a164.sql 3.0079572200775146
3 4 1747322330.4279044 sample_queries/q40_2a8120.sql 14.527836561203003
3 5 1747322333.8091981 sample_queries/q36_7a136.sql 3.381256580352783
3 6 1747322335.9118397 sample_queries/q8_6a505.sql 2.1026058197021484
3 7 1747322339.3588345 sample_queries/q34_1a275.sql 3.4469618797302246
3 8 1747322341.4465704 sample_queries/q8_6a505.sql 2.0877010822296143
3 9 1747322346.6787498 sample_queries/q18_7a103.sql 5.232143878936768
3 10 1747322349.6356578 sample_queries/q17_7a164.sql 2.9568698406219482
3 11 1747322351.5251794 sample_queries/q18_7a103.sql 1.8894860744476318
3 12 1747322351.7123315 sample_queries/q24_32a.sql 0.18711543083190918
3 13 1747322354.2559822 sample_queries/q9_5a48.sql 2.543609142303467
3 14 1747322356.1599445 sample_queries/q18_7a103.sql 1.9039270877838135
3 15 1747322357.0758948 sample_queries/q27_3c.sql 0.9159088134765625
3 16 1747322358.6888351 sample_queries/q12_17a.sql 1.6129066944122314
3 17 1747322360.512165 sample_queries/q11_17e.sql 1.8232934474945068
3 18 1747322364.2341838 sample_queries/q2_8a82.sql 3.7219786643981934
3 19 1747322367.3091671 sample_queries/q32_2a493.sql 3.0749118328094482
3 20 1747322368.4408102 sample_queries/q29_6e.sql 1.1316087245941162
3 21 1747322370.9133744 sample_queries/q6_16b.sql 2.4725289344787598
3 22 1747322372.8176274 sample_queries/q4_8a122.sql 1.9042165279388428
3 23 1747322374.753715 sample_queries/q8_6a505.sql 1.9360532760620117
3 24 1747322381.2870026 sample_queries/q31_2a39.sql 6.533253908157349
Initial input channels: 10
Epoch 0 training loss: 0.7719707582145929
Epoch 15 training loss: 0.3705498017370701
Epoch 30 training loss: 0.34556588530540466
Epoch 45 training loss: 0.36668939888477325
Epoch 60 training loss: 0.48139292746782303
Epoch 75 training loss: 0.41490447893738747
Epoch 90 training loss: 0.22200774669181556
Epoch 105 training loss: 0.5744435600936413
Epoch 120 training loss: 0.5043730437755585
Epoch 135 training loss: 0.3814396299421787
Epoch 150 training loss: 0.39353254064917564
Epoch 165 training loss: 0.23458459321409464
Epoch 180 training loss: 0.34458987787365913
Epoch 195 training loss: 0.3154842397198081
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
4 0 1747322398.0577033 sample_queries/q1_8a463.sql 5.930042266845703
4 1 1747322399.7445529 sample_queries/q15_18a.sql 1.6868090629577637
4 2 1747322405.5451412 sample_queries/q38_2a1870.sql 5.800554513931274
4 3 1747322406.3184848 sample_queries/q16_26c.sql 0.7733111381530762
4 4 1747322413.1231031 sample_queries/q10_2a265.sql 6.804585695266724
4 5 1747322414.6923912 sample_queries/q35_1a1508.sql 1.569253921508789
4 6 1747322417.4522188 sample_queries/q14_6a349.sql 2.7597923278808594
4 7 1747322417.692249 sample_queries/q32_2a493.sql 0.23999691009521484
4 8 1747322420.216349 sample_queries/q18_7a103.sql 2.5240676403045654
4 9 1747322422.8261013 sample_queries/q26_2a274.sql 2.6097166538238525
4 10 1747322426.20732 sample_queries/q10_2a265.sql 3.381183624267578
4 11 1747322426.3344798 sample_queries/q24_32a.sql 0.12712502479553223
4 12 1747322453.3079572 sample_queries/q22_8a27.sql 26.973441123962402
4 13 1747322458.7966695 sample_queries/q1_8a463.sql 5.488665580749512
4 14 1747322460.2727644 sample_queries/q15_18a.sql 1.4760611057281494
4 15 1747322460.8028545 sample_queries/q31_2a39.sql 0.5300545692443848
4 16 1747322462.3596463 sample_queries/q35_1a1508.sql 1.5567569732666016
4 17 1747322464.0713747 sample_queries/q17_7a164.sql 1.7116925716400146
4 18 1747322468.7714233 sample_queries/q11_17e.sql 4.700015068054199
4 19 1747322474.2323833 sample_queries/q1_8a463.sql 5.460925579071045
4 20 1747322479.1440728 sample_queries/q35_1a1508.sql 4.911653757095337
4 21 1747322484.5022037 sample_queries/q1_8a463.sql 5.358097314834595
4 22 1747322485.1216989 sample_queries/q33_2a156.sql 0.6194500923156738
4 23 1747322489.7174962 sample_queries/q12_17a.sql 4.595759153366089
4 24 1747322492.4968822 sample_queries/q36_7a136.sql 2.779351234436035
Initial input channels: 10
Epoch 0 training loss: 1.568657474219799
Epoch 15 training loss: 0.42280748188495637
Epoch 30 training loss: 0.352689591050148
Epoch 45 training loss: 0.48015573173761367
Epoch 60 training loss: 0.3666408061981201
Epoch 75 training loss: 0.40745193511247635
Epoch 90 training loss: 0.3499141339212656
Epoch 105 training loss: 0.46604844778776167
Epoch 120 training loss: 0.48454375863075255
Epoch 135 training loss: 0.5971502631902694
Epoch 150 training loss: 0.33971053957939146
Epoch 165 training loss: 0.4265536546707153
Epoch 180 training loss: 0.29287107214331626
Epoch 195 training loss: 0.4223002910614014
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
5 0 1747322506.5432794 sample_queries/q15_18a.sql 1.4956319332122803
5 1 1747322512.3952475 sample_queries/q18_7a103.sql 5.851933002471924
5 2 1747322518.3887875 sample_queries/q37_2a1291.sql 5.993504762649536
5 3 1747322518.9831674 sample_queries/q28_13a.sql 0.594346284866333
5 4 1747322519.589693 sample_queries/q33_2a156.sql 0.6064915657043457
5 5 1747322519.7519352 sample_queries/q20_24b.sql 0.16220927238464355
5 6 1747322531.1933434 sample_queries/q34_1a275.sql 11.441375017166138
5 7 1747322531.352954 sample_queries/q20_24b.sql 0.15957427024841309
5 8 1747322531.949749 sample_queries/q28_13a.sql 0.5967578887939453
5 9 1747322534.7625735 sample_queries/q26_2a274.sql 2.8127851486206055
5 10 1747322547.219893 sample_queries/q6_16b.sql 12.457281827926636
5 11 1747322582.4677455 sample_queries/q38_2a1870.sql 35.24781847000122
5 12 1747322586.152892 sample_queries/q2_8a82.sql 3.6851110458374023
5 13 1747322625.7889304 sample_queries/q40_2a8120.sql 39.6360023021698
5 14 1747322626.0338845 sample_queries/q32_2a493.sql 0.244917631149292
5 15 1747322629.1637406 sample_queries/q36_7a136.sql 3.1298210620880127
5 16 1747322629.2306209 sample_queries/q29_6e.sql 0.06684327125549316
5 17 1747322629.8396566 sample_queries/q31_2a39.sql 0.6089792251586914
5 18 1747322633.3818667 sample_queries/q18_7a103.sql 3.542175054550171
5 19 1747322651.7115555 sample_queries/q7_7a48.sql 18.329651832580566
5 20 1747322652.5068169 sample_queries/q33_2a156.sql 0.79522705078125
5 21 1747322654.2323542 sample_queries/q3_7a99.sql 1.725503921508789
5 22 1747322654.4642944 sample_queries/q32_2a493.sql 0.2319028377532959
5 23 1747322657.6408958 sample_queries/q10_2a265.sql 3.1765682697296143
5 24 1747322660.4339597 sample_queries/q14_6a349.sql 2.7930307388305664
Initial input channels: 10
Epoch 0 training loss: 0.9148470542647622
Epoch 15 training loss: 0.3318991898135705
Epoch 30 training loss: 0.3498140661553903
Epoch 45 training loss: 0.4200158322399313
Epoch 60 training loss: 0.35193329778584564
Epoch 75 training loss: 0.39127264375036414
Epoch 90 training loss: 0.368499066342007
Epoch 105 training loss: 0.35608248819004407
Epoch 120 training loss: 0.4504467411474748
Epoch 135 training loss: 0.3736067089167508
Epoch 150 training loss: 0.34828450733965094
Epoch 165 training loss: 0.3391226204958829
Epoch 180 training loss: 0.28287480907006696
Epoch 195 training loss: 0.39144749533046375
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
6 0 1747322686.1094115 sample_queries/q5_8a423.sql 11.778864860534668
6 1 1747322692.066893 sample_queries/q35_1a1508.sql 5.957448482513428
6 2 1747322700.3942943 sample_queries/q6_16b.sql 8.327366590499878
6 3 1747322702.3397431 sample_queries/q5_8a423.sql 1.9454071521759033
6 4 1747322715.890397 sample_queries/q19_2a471.sql 13.550616264343262
6 5 1747322720.6819313 sample_queries/q9_5a48.sql 4.7915003299713135
6 6 1747322720.9173932 sample_queries/q32_2a493.sql 0.2354276180267334
6 7 1747322730.7803726 sample_queries/q37_2a1291.sql 9.862943887710571
6 8 1747322730.8386097 sample_queries/q29_6e.sql 0.05820274353027344
6 9 1747322736.4795163 sample_queries/q1_8a463.sql 5.640864372253418
6 10 1747322737.0842068 sample_queries/q28_13a.sql 0.6046500205993652
6 11 1747322737.319226 sample_queries/q27_3c.sql 0.23498320579528809
6 12 1747322737.4901712 sample_queries/q20_24b.sql 0.17090916633605957
6 13 1747322739.6880367 sample_queries/q22_8a27.sql 2.1978237628936768
6 14 1747322745.6414156 sample_queries/q26_2a274.sql 5.953337907791138
6 15 1747322747.7647998 sample_queries/q1_8a463.sql 2.1233510971069336
6 16 1747322749.3271723 sample_queries/q15_18a.sql 1.5623359680175781
6 17 1747322751.0413647 sample_queries/q3_7a99.sql 1.7141532897949219
6 18 1747322753.5434268 sample_queries/q23_19d.sql 2.5020205974578857
6 19 1747322756.18843 sample_queries/q23_19d.sql 2.6449687480926514
6 20 1747322764.5065513 sample_queries/q39_2a2781.sql 8.318089962005615
6 21 1747322770.3176417 sample_queries/q35_1a1508.sql 5.811053037643433
6 22 1747322772.9172244 sample_queries/q26_2a274.sql 2.5995500087738037
6 23 1747322773.8083532 sample_queries/q21_2a396.sql 0.8910956382751465
6 24 1747322785.0888696 sample_queries/q39_2a2781.sql 11.28048300743103
Initial input channels: 10
Epoch 0 training loss: 1.2423313105335603
Epoch 15 training loss: 0.5620850977989343
Epoch 30 training loss: 0.4049616436688946
Epoch 45 training loss: 0.49858367457412756
Epoch 60 training loss: 0.49432046023698956
Epoch 75 training loss: 0.4134297388104292
Epoch 90 training loss: 0.494693317092382
Epoch 105 training loss: 0.4651501135757336
Epoch 120 training loss: 0.6218603637356025
Epoch 135 training loss: 0.40228159782978207
Epoch 150 training loss: 0.31805094281354773
Epoch 165 training loss: 0.24951367653333223
Epoch 180 training loss: 0.2248116318996136
Epoch 195 training loss: 0.2503107293294026
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
7 0 1747322806.8416517 sample_queries/q19_2a471.sql 5.822175741195679
7 1 1747322807.874581 sample_queries/q27_3c.sql 1.032895803451538
7 2 1747322813.7442842 sample_queries/q23_19d.sql 5.869669675827026
7 3 1747322816.3917935 sample_queries/q9_5a48.sql 2.6474761962890625
7 4 1747322822.3829975 sample_queries/q19_2a471.sql 5.991170883178711
7 5 1747322822.9056945 sample_queries/q31_2a39.sql 0.5226640701293945
7 6 1747322864.951141 sample_queries/q2_8a82.sql 42.04541277885437
7 7 1747322866.7808087 sample_queries/q11_17e.sql 1.8296353816986084
7 8 1747322869.8627064 sample_queries/q17_7a164.sql 3.081864595413208
7 9 1747322871.5632098 sample_queries/q5_8a423.sql 1.700469970703125
7 10 1747322880.2020464 sample_queries/q39_2a2781.sql 8.638799667358398
7 11 1747322882.5484333 sample_queries/q18_7a103.sql 2.346353769302368
7 12 1747322885.2462697 sample_queries/q17_7a164.sql 2.6978025436401367
7 13 1747322888.6858153 sample_queries/q7_7a48.sql 3.4395127296447754
7 14 1747322889.2253249 sample_queries/q31_2a39.sql 0.5394771099090576
7 15 1747322892.5151174 sample_queries/q36_7a136.sql 3.28975772857666
7 16 1747322894.9519835 sample_queries/q6_16b.sql 2.4368255138397217
7 17 1747322897.0698726 sample_queries/q1_8a463.sql 2.1178557872772217
7 18 1747322897.8376987 sample_queries/q28_13a.sql 0.7677919864654541
7 19 1747322900.2457454 sample_queries/q6_16b.sql 2.4080023765563965
7 20 1747322902.9794123 sample_queries/q17_7a164.sql 2.7336294651031494
7 21 1747322905.0991848 sample_queries/q1_8a463.sql 2.119738817214966
7 22 1747322906.7488275 sample_queries/q5_8a423.sql 1.6496069431304932
7 23 1747322916.1665685 sample_queries/q40_2a8120.sql 9.417707681655884
7 24 1747322916.2275026 sample_queries/q29_6e.sql 0.06089973449707031
Initial input channels: 10
Epoch 0 training loss: 0.9930837621291478
Epoch 15 training loss: 0.42438375105460485
Epoch 30 training loss: 0.37468463778495786
Epoch 45 training loss: 0.3475879887739817
Epoch 60 training loss: 0.38666724661986035
Epoch 75 training loss: 0.3579284866650899
Epoch 90 training loss: 0.36270516415437065
Epoch 105 training loss: 0.3482868820428848
Epoch 120 training loss: 0.38016222367684044
Epoch 135 training loss: 0.392135031769673
Epoch 150 training loss: 0.4560033053159714
Epoch 165 training loss: 0.45710398952166237
Epoch 180 training loss: 0.35267554968595505
Epoch 195 training loss: 0.46275484065214795
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
8 0 1747322936.2141294 sample_queries/q15_18a.sql 1.421769142150879
8 1 1747322949.0050073 sample_queries/q40_2a8120.sql 12.790843963623047
8 2 1747322949.8929017 sample_queries/q21_2a396.sql 0.8878581523895264
8 3 1747322959.543926 sample_queries/q38_2a1870.sql 9.650989055633545
8 4 1747322965.1618245 sample_queries/q1_8a463.sql 5.617863655090332
8 5 1747322967.3867097 sample_queries/q16_26c.sql 2.224834680557251
8 6 1747322973.4080136 sample_queries/q35_1a1508.sql 6.021268129348755
8 7 1747322973.4549074 sample_queries/q24_32a.sql 0.04685807228088379
8 8 1747322973.5020099 sample_queries/q24_32a.sql 0.04706931114196777
8 9 1747322979.8593156 sample_queries/q26_2a274.sql 6.357270240783691
8 10 1747322980.895729 sample_queries/q27_3c.sql 1.0363805294036865
8 11 1747322981.93774 sample_queries/q27_3c.sql 1.0419678688049316
8 12 1747323002.02494 sample_queries/q33_2a156.sql 20.08716654777527
8 13 1747323004.7355056 sample_queries/q14_6a349.sql 2.7105302810668945
8 14 1747323014.3409588 sample_queries/q35_1a1508.sql 9.605412721633911
8 15 1747323014.9864166 sample_queries/q28_13a.sql 0.6454153060913086
8 16 1747323096.5935867 sample_queries/q3_7a99.sql 81.60713338851929
8 17 1747323096.7560306 sample_queries/q20_24b.sql 0.16240715980529785
8 18 1747323100.501146 sample_queries/q34_1a275.sql 3.745082378387451
8 19 1747323103.0952451 sample_queries/q18_7a103.sql 2.5940563678741455
8 20 1747323112.4035378 sample_queries/q35_1a1508.sql 9.308255672454834
8 21 1747323118.3919098 sample_queries/q35_1a1508.sql 5.988337278366089
8 22 1747323194.9742308 sample_queries/q2_8a82.sql 76.58228540420532
8 23 1747323197.1675792 sample_queries/q28_13a.sql 2.193305253982544
8 24 1747323203.0905871 sample_queries/q8_6a505.sql 5.922961473464966
Initial input channels: 10
Epoch 0 training loss: 11.772266984917223
Epoch 15 training loss: 0.5364562161266804
Epoch 30 training loss: 0.5145311090163887
Epoch 45 training loss: 0.396349819842726
Epoch 60 training loss: 0.40600864170119166
Epoch 75 training loss: 0.4049169756472111
Epoch 90 training loss: 0.4096963657066226
Epoch 105 training loss: 0.26551481569185853
Epoch 120 training loss: 0.37926321709528565
Epoch 135 training loss: 0.3486379021778703
Epoch 150 training loss: 0.3563347514718771
Epoch 165 training loss: 0.30639735935255885
Epoch 180 training loss: 0.3155284053646028
Epoch 195 training loss: 0.3173594046384096
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
9 0 1747323228.654822 sample_queries/q23_19d.sql 2.4763128757476807
9 1 1747323230.1467915 sample_queries/q13_7a121.sql 1.4919347763061523
9 2 1747323255.496776 sample_queries/q22_8a27.sql 25.349952220916748
9 3 1747323260.4540122 sample_queries/q11_17e.sql 4.957203149795532
9 4 1747323262.1223066 sample_queries/q5_8a423.sql 1.668257474899292
9 5 1747323271.6041873 sample_queries/q40_2a8120.sql 9.48184609413147
9 6 1747323273.3255427 sample_queries/q17_7a164.sql 1.7213139533996582
9 7 1747323273.946732 sample_queries/q33_2a156.sql 0.6211564540863037
9 8 1747323279.1709867 sample_queries/q18_7a103.sql 5.224220275878906
9 9 1747323290.94081 sample_queries/q4_8a122.sql 11.769788026809692
9 10 1747323293.023607 sample_queries/q25_13d.sql 2.0827584266662598
9 11 1747323297.704072 sample_queries/q12_17a.sql 4.680417776107788
9 12 1747323298.31459 sample_queries/q33_2a156.sql 0.6104819774627686
9 13 1747323299.9123714 sample_queries/q13_7a121.sql 1.5977473258972168
9 14 1747323301.5695093 sample_queries/q5_8a423.sql 1.6571040153503418
9 15 1747323302.7339556 sample_queries/q7_7a48.sql 1.1644103527069092
9 16 1747323302.9622085 sample_queries/q32_2a493.sql 0.22821950912475586
9 17 1747323316.624442 sample_queries/q19_2a471.sql 13.662199974060059
9 18 1747323326.1110554 sample_queries/q38_2a1870.sql 9.486577987670898
9 19 1747323330.7921138 sample_queries/q11_17e.sql 4.681024789810181
9 20 1747323333.2935295 sample_queries/q14_6a349.sql 2.5013809204101562
9 21 1747323334.2293775 sample_queries/q21_2a396.sql 0.9358131885528564
9 22 1747323338.7967653 sample_queries/q12_17a.sql 4.567352294921875
9 23 1747323354.211744 sample_queries/q4_8a122.sql 15.414944648742676
9 24 1747323357.6537476 sample_queries/q34_1a275.sql 3.4419679641723633
Initial input channels: 10
Epoch 0 training loss: 0.9508552882406447
Epoch 15 training loss: 0.38437757682469154
Epoch 30 training loss: 0.3936316658380545
Epoch 45 training loss: 0.42504319962528014
Epoch 60 training loss: 0.39608670336504775
Epoch 75 training loss: 0.28194644302129745
Epoch 90 training loss: 0.42868904241671163
Epoch 105 training loss: 0.44774773261613315
Epoch 120 training loss: 0.30948459170758724
Epoch 135 training loss: 0.3626969255920913
Epoch 150 training loss: 0.4210498150851991
Epoch 165 training loss: 0.4268827007876502
Epoch 180 training loss: 0.308894372648663
Epoch 195 training loss: 0.328885021722979
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
10 0 1747323386.459458 sample_queries/q3_7a99.sql 3.033174991607666
10 1 1747323387.887737 sample_queries/q34_1a275.sql 1.4282464981079102
10 2 1747323390.4925828 sample_queries/q18_7a103.sql 2.6048121452331543
10 3 1747323390.6620898 sample_queries/q20_24b.sql 0.16947102546691895
10 4 1747323390.8959272 sample_queries/q32_2a493.sql 0.23380351066589355
10 5 1747323399.291257 sample_queries/q39_2a2781.sql 8.395297765731812
10 6 1747323403.1221712 sample_queries/q33_2a156.sql 3.830880641937256
10 7 1747323403.9717531 sample_queries/q21_2a396.sql 0.8495485782623291
10 8 1747323415.2167866 sample_queries/q39_2a2781.sql 11.245000123977661
10 9 1747323417.2360067 sample_queries/q25_13d.sql 2.0191876888275146
10 10 1747323421.0677345 sample_queries/q33_2a156.sql 3.831695795059204
10 11 1747323430.1610782 sample_queries/q40_2a8120.sql 9.09330940246582
10 12 1747323431.3941364 sample_queries/q34_1a275.sql 1.2330238819122314
10 13 1747323432.2974854 sample_queries/q27_3c.sql 0.9033143520355225
10 14 1747323432.808768 sample_queries/q31_2a39.sql 0.5112507343292236
10 15 1747323435.3046303 sample_queries/q18_7a103.sql 2.4958293437957764
10 16 1747323437.8750548 sample_queries/q26_2a274.sql 2.570390462875366
10 17 1747323450.3795855 sample_queries/q40_2a8120.sql 12.504497289657593
10 18 1747323451.1808825 sample_queries/q28_13a.sql 0.8012640476226807
10 19 1747323452.3818269 sample_queries/q10_2a265.sql 1.200911283493042
10 20 1747323455.4223828 sample_queries/q3_7a99.sql 3.040523052215576
10 21 1747323457.6253438 sample_queries/q16_26c.sql 2.202928066253662
10 22 1747323459.5417707 sample_queries/q11_17e.sql 1.916388988494873
10 23 1747323460.0497625 sample_queries/q31_2a39.sql 0.5079593658447266
10 24 1747323469.5874243 sample_queries/q38_2a1870.sql 9.537628412246704
Initial input channels: 10
Epoch 0 training loss: 2.584310713175096
Epoch 15 training loss: 0.3980017322066583
Epoch 30 training loss: 0.3563787450915889
Epoch 45 training loss: 0.3114254784427191
Epoch 60 training loss: 0.37965883079328033
Epoch 75 training loss: 0.3935120960599498
Epoch 90 training loss: 0.3479451231266323
Epoch 105 training loss: 0.37866599936234324
Epoch 120 training loss: 0.374910568720416
Epoch 135 training loss: 0.37011661419742986
Epoch 150 training loss: 0.3384056816759862
Epoch 165 training loss: 0.3477356410340259
Epoch 180 training loss: 0.3312874403046934
Epoch 195 training loss: 0.4213762928388621
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
11 0 1747323498.557087 sample_queries/q14_6a349.sql 2.701875686645508
11 1 1747323501.1716 sample_queries/q26_2a274.sql 2.6144769191741943
11 2 1747323501.4198122 sample_queries/q27_3c.sql 0.24817824363708496
11 3 1747323504.0649981 sample_queries/q14_6a349.sql 2.64514422416687
11 4 1747323504.7512398 sample_queries/q33_2a156.sql 0.6862096786499023
11 5 1747323513.608262 sample_queries/q1_8a463.sql 8.85698914527893
11 6 1747323515.211448 sample_queries/q35_1a1508.sql 1.6031501293182373
11 7 1747323515.2693443 sample_queries/q29_6e.sql 0.05785560607910156
11 8 1747323515.3298092 sample_queries/q29_6e.sql 0.060426950454711914
11 9 1747323521.432309 sample_queries/q19_2a471.sql 6.102467060089111
11 10 1747323527.1508307 sample_queries/q38_2a1870.sql 5.718486785888672
11 11 1747323530.3292918 sample_queries/q10_2a265.sql 3.178428888320923
11 12 1747323530.769536 sample_queries/q21_2a396.sql 0.44021129608154297
11 13 1747323533.4974868 sample_queries/q14_6a349.sql 2.727917432785034
11 14 1747323536.0608468 sample_queries/q13_7a121.sql 2.5633277893066406
11 15 1747323539.1033742 sample_queries/q10_2a265.sql 3.0424952507019043
11 16 1747323539.1612098 sample_queries/q29_6e.sql 0.05780196189880371
11 17 1747323548.6352901 sample_queries/q12_17a.sql 9.474036931991577
11 18 1747323551.393919 sample_queries/q14_6a349.sql 2.7585928440093994
11 19 1747323554.200509 sample_queries/q26_2a274.sql 2.806553363800049
11 20 1747323555.8335829 sample_queries/q35_1a1508.sql 1.633033275604248
11 21 1747323559.6682103 sample_queries/q18_7a103.sql 3.8345935344696045
11 22 1747323560.9972718 sample_queries/q34_1a275.sql 1.3290207386016846
11 23 1747323562.7631583 sample_queries/q30_18c.sql 1.7658500671386719
11 24 1747323574.1110287 sample_queries/q6_16b.sql 11.347835779190063
Initial input channels: 10
Epoch 0 training loss: 1.5376132194484984
Epoch 15 training loss: 0.47471809422685984
Epoch 30 training loss: 0.3547955013456799
Epoch 45 training loss: 0.31544135439963567
Epoch 60 training loss: 0.387614274308795
Epoch 75 training loss: 0.3889504028927712
Epoch 90 training loss: 0.41535174527338575
Epoch 105 training loss: 0.3704009595371428
Epoch 120 training loss: 0.3744329061536562
Epoch 135 training loss: 0.31907178177720025
Epoch 150 training loss: 0.4202562861499332
Epoch 165 training loss: 0.4015763597119422
Epoch 180 training loss: 0.44445634438168435
Epoch 195 training loss: 0.3613806040514083
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
12 0 1747323606.0183148 sample_queries/q3_7a99.sql 3.0098142623901367
12 1 1747323614.3204327 sample_queries/q9_5a48.sql 8.302081108093262
12 2 1747323616.7432394 sample_queries/q23_19d.sql 2.4227678775787354
12 3 1747323618.5881617 sample_queries/q5_8a423.sql 1.8448882102966309
12 4 1747323619.4807656 sample_queries/q21_2a396.sql 0.8925693035125732
12 5 1747323619.6431332 sample_queries/q20_24b.sql 0.16232585906982422
12 6 1747323630.8526456 sample_queries/q26_2a274.sql 11.209472179412842
12 7 1747323632.1070845 sample_queries/q10_2a265.sql 1.254406213760376
12 8 1747323635.0744529 sample_queries/q36_7a136.sql 2.967332363128662
12 9 1747323637.389048 sample_queries/q13_7a121.sql 2.314554214477539
12 10 1747323659.2608569 sample_queries/q32_2a493.sql 21.8717622756958
12 11 1747323664.5106504 sample_queries/q12_17a.sql 5.249753713607788
12 12 1747323678.916782 sample_queries/q26_2a274.sql 14.406089544296265
12 13 1747323681.1263335 sample_queries/q1_8a463.sql 2.209517478942871
12 14 1747323684.176672 sample_queries/q7_7a48.sql 3.050305128097534
12 15 1747323684.3537157 sample_queries/q20_24b.sql 0.17700648307800293
12 16 1747323686.8279355 sample_queries/q23_19d.sql 2.4741859436035156
12 17 1747323687.8136876 sample_queries/q21_2a396.sql 0.9857189655303955
12 18 1747323690.2536232 sample_queries/q14_6a349.sql 2.4398999214172363
12 19 1747323690.9548104 sample_queries/q33_2a156.sql 0.7011556625366211
12 20 1747323691.7889426 sample_queries/q21_2a396.sql 0.8340988159179688
12 21 1747323693.990394 sample_queries/q1_8a463.sql 2.20141339302063
12 22 1747323694.8526566 sample_queries/q21_2a396.sql 0.8622279167175293
12 23 1747323697.286187 sample_queries/q23_19d.sql 2.433495044708252
12 24 1747323706.8444571 sample_queries/q39_2a2781.sql 9.558237552642822
Initial input channels: 10
Epoch 0 training loss: 1.629432319917462
Epoch 15 training loss: 0.4460557804188945
Epoch 30 training loss: 0.453348119827834
Epoch 45 training loss: 0.37119493227113376
Epoch 60 training loss: 0.4115021073005416
Epoch 75 training loss: 0.3806927007707683
Epoch 90 training loss: 0.3917635076425292
Epoch 105 training loss: 0.4123720499602231
Epoch 120 training loss: 0.37672286345200107
Epoch 135 training loss: 0.45459807867353613
Epoch 150 training loss: 0.3330617885697972
Epoch 165 training loss: 0.33000391044399957
Epoch 180 training loss: 0.3702213872562755
Epoch 195 training loss: 0.43214802647178824
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
13 0 1747323740.3858273 sample_queries/q18_7a103.sql 2.6771934032440186
13 1 1747323746.5270047 sample_queries/q14_6a349.sql 6.141134977340698
13 2 1747323772.7265766 sample_queries/q22_8a27.sql 26.199538230895996
13 3 1747323774.6893537 sample_queries/q30_18c.sql 1.962742567062378
13 4 1747323774.8541603 sample_queries/q20_24b.sql 0.16476655006408691
13 5 1747323780.9903357 sample_queries/q14_6a349.sql 6.136122226715088
13 6 1747323783.417217 sample_queries/q23_19d.sql 2.426846981048584
13 7 1747323789.1447926 sample_queries/q8_6a505.sql 5.727540969848633
13 8 1747323795.260391 sample_queries/q5_8a423.sql 6.1155641078948975
13 9 1747323798.0589933 sample_queries/q26_2a274.sql 2.7985661029815674
13 10 1747323800.7149656 sample_queries/q26_2a274.sql 2.655935764312744
13 11 1747323801.671627 sample_queries/q16_26c.sql 0.9566256999969482
13 12 1747323817.1012418 sample_queries/q34_1a275.sql 15.429579257965088
13 13 1747323820.1270676 sample_queries/q36_7a136.sql 3.025789499282837
13 14 1747323826.0750139 sample_queries/q1_8a463.sql 5.94791054725647
13 15 1747323826.7122893 sample_queries/q33_2a156.sql 0.6372370719909668
13 16 1747323829.5112727 sample_queries/q14_6a349.sql 2.7989485263824463
13 17 1747323830.0362408 sample_queries/q21_2a396.sql 0.5249321460723877
13 18 1747323830.2068348 sample_queries/q20_24b.sql 0.17055797576904297
13 19 1747323835.5838637 sample_queries/q39_2a2781.sql 5.376986980438232
13 20 1747323835.8132403 sample_queries/q32_2a493.sql 0.22933578491210938
13 21 1747323835.8704627 sample_queries/q29_6e.sql 0.05718731880187988
13 22 1747323838.417799 sample_queries/q26_2a274.sql 2.547302722930908
13 23 1747323840.1605322 sample_queries/q30_18c.sql 1.7426962852478027
13 24 1747323840.8567076 sample_queries/q28_13a.sql 0.6961410045623779
Initial input channels: 10
Epoch 0 training loss: 1.1609849060575168
Epoch 15 training loss: 0.35555959058304626
Epoch 30 training loss: 0.3754071593284607
Epoch 45 training loss: 0.4145694753775994
Epoch 60 training loss: 0.37988459194699925
Epoch 75 training loss: 0.39061355280379456
Epoch 90 training loss: 0.3890999046464761
Epoch 105 training loss: 0.3697909101222952
Epoch 120 training loss: 0.3572164460395773
Epoch 135 training loss: 0.3548766163488229
Epoch 150 training loss: 0.34412055400510627
Epoch 165 training loss: 0.3421830494577686
Epoch 180 training loss: 0.4177823029458523
Epoch 195 training loss: 0.4115292307299872
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
14 0 1747323882.441862 sample_queries/q1_8a463.sql 8.931045055389404
14 1 1747323888.4543996 sample_queries/q40_2a8120.sql 6.012501001358032
14 2 1747323893.5974376 sample_queries/q39_2a2781.sql 5.143003463745117
14 3 1747323893.6470327 sample_queries/q24_32a.sql 0.04956388473510742
14 4 1747323894.5373049 sample_queries/q25_13d.sql 0.8902342319488525
14 5 1747323896.0330884 sample_queries/q7_7a48.sql 1.4957497119903564
14 6 1747323899.2110472 sample_queries/q10_2a265.sql 3.177924871444702
14 7 1747323905.0272381 sample_queries/q38_2a1870.sql 5.816157579421997
14 8 1747323907.308893 sample_queries/q16_26c.sql 2.2816219329833984
14 9 1747323908.0348103 sample_queries/q17_7a164.sql 0.7258846759796143
14 10 1747323936.8471222 sample_queries/q22_8a27.sql 28.812278270721436
14 11 1747323937.0842001 sample_queries/q32_2a493.sql 0.23704266548156738
14 12 1747323937.5232975 sample_queries/q21_2a396.sql 0.43903636932373047
14 13 1747323937.581545 sample_queries/q29_6e.sql 0.058213233947753906
14 14 1747323937.6382458 sample_queries/q29_6e.sql 0.05666065216064453
14 15 1747323949.4405348 sample_queries/q6_16b.sql 11.802248239517212
14 16 1747323949.9767704 sample_queries/q31_2a39.sql 0.5362005233764648
14 17 1747323955.5241647 sample_queries/q1_8a463.sql 5.547359228134155
14 18 1747323956.1442556 sample_queries/q33_2a156.sql 0.6200504302978516
14 19 1747323957.7283018 sample_queries/q30_18c.sql 1.5839393138885498
14 20 1747323958.1756647 sample_queries/q21_2a396.sql 0.4473264217376709
14 21 1747323962.1009526 sample_queries/q36_7a136.sql 3.925253391265869
14 22 1747323965.2267392 sample_queries/q10_2a265.sql 3.12575101852417
14 23 1747323970.0387468 sample_queries/q9_5a48.sql 4.8119728565216064
14 24 1747323985.5191638 sample_queries/q4_8a122.sql 15.48038101196289
Initial input channels: 10
Epoch 0 training loss: 1.4593886683881283
Epoch 15 training loss: 0.38643553376197814
Epoch 30 training loss: 0.35794758319854736
Epoch 45 training loss: 0.3540027570724487
Epoch 60 training loss: 0.37769667863845824
Epoch 75 training loss: 0.3771649581193924
Epoch 90 training loss: 0.33600072413682935
Epoch 105 training loss: 0.3337809094786644
Epoch 120 training loss: 0.36868259504437445
Epoch 135 training loss: 0.38468064963817594
Epoch 150 training loss: 0.46102030962705615
Epoch 165 training loss: 0.3511264687776566
Epoch 180 training loss: 0.35684618562459947
Epoch 195 training loss: 0.27283263579010963
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
15 0 1747324026.1654 sample_queries/q12_17a.sql 6.063976287841797
15 1 1747324029.4418123 sample_queries/q36_7a136.sql 3.2763750553131104
15 2 1747324030.9468946 sample_queries/q15_18a.sql 1.505040168762207
15 3 1747324035.6858027 sample_queries/q9_5a48.sql 4.73887300491333
15 4 1747324035.9231215 sample_queries/q27_3c.sql 0.23728442192077637
15 5 1747324045.251412 sample_queries/q37_2a1291.sql 9.32825517654419
15 6 1747324047.403167 sample_queries/q1_8a463.sql 2.1517186164855957
15 7 1747324050.8332584 sample_queries/q34_1a275.sql 3.430049419403076
15 8 1747324055.6112468 sample_queries/q11_17e.sql 4.7779529094696045
15 9 1747324060.9651814 sample_queries/q39_2a2781.sql 5.353883266448975
15 10 1747324063.094265 sample_queries/q1_8a463.sql 2.1290478706359863
15 11 1747324065.3816457 sample_queries/q16_26c.sql 2.287346839904785
15 12 1747324068.0099115 sample_queries/q26_2a274.sql 2.6282317638397217
15 13 1747324068.0677507 sample_queries/q29_6e.sql 0.05780506134033203
15 14 1747324074.400904 sample_queries/q11_17e.sql 6.333114385604858
15 15 1747324079.9859648 sample_queries/q16_26c.sql 5.585025310516357
15 16 1747324080.1176314 sample_queries/q24_32a.sql 0.13162446022033691
15 17 1747324115.2260315 sample_queries/q2_8a82.sql 35.10836672782898
15 18 1747324120.9736948 sample_queries/q35_1a1508.sql 5.74762749671936
15 19 1747324122.4497173 sample_queries/q15_18a.sql 1.4759883880615234
15 20 1747324124.9994624 sample_queries/q3_7a99.sql 2.5497117042541504
15 21 1747324127.7923765 sample_queries/q23_19d.sql 2.7928807735443115
15 22 1747324132.9910233 sample_queries/q9_5a48.sql 5.198614835739136
15 23 1747324135.419454 sample_queries/q23_19d.sql 2.428394317626953
15 24 1747324138.0524669 sample_queries/q26_2a274.sql 2.6329615116119385
Initial input channels: 10
Epoch 0 training loss: 2.1375831357306905
Epoch 15 training loss: 0.5486234956317477
Epoch 30 training loss: 0.41067225389458517
Epoch 45 training loss: 0.27974995553355525
Epoch 60 training loss: 0.33019950861732167
Epoch 75 training loss: 0.29701829743054176
Epoch 90 training loss: 0.40837153130107456
Epoch 105 training loss: 0.31226749980339297
Epoch 120 training loss: 0.3610916098717738
Epoch 135 training loss: 0.29323389422562385
Epoch 150 training loss: 0.23706311070256764
Epoch 165 training loss: 0.3984404255256609
Epoch 180 training loss: 0.2886451599360616
Epoch 195 training loss: 0.32236864466082166
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
16 0 1747324179.2775614 sample_queries/q23_19d.sql 5.762224912643433
16 1 1747324180.2024975 sample_queries/q13_7a121.sql 0.9249007701873779
16 2 1747324183.6653292 sample_queries/q7_7a48.sql 3.462796449661255
16 3 1747324184.174606 sample_queries/q31_2a39.sql 0.5092422962188721
16 4 1747324190.2700975 sample_queries/q38_2a1870.sql 6.095456123352051
16 5 1747324192.4570818 sample_queries/q16_26c.sql 2.186950206756592
16 6 1747324193.0429935 sample_queries/q28_13a.sql 0.58587646484375
16 7 1747324193.2054312 sample_queries/q20_24b.sql 0.16240310668945312
16 8 1747324198.7593098 sample_queries/q1_8a463.sql 5.553846120834351
16 9 1747324198.8871326 sample_queries/q24_32a.sql 0.12778782844543457
16 10 1747324199.497347 sample_queries/q33_2a156.sql 0.610175371170044
16 11 1747324200.6125603 sample_queries/q27_3c.sql 1.115177869796753
16 12 1747324202.2169516 sample_queries/q35_1a1508.sql 1.6043446063995361
16 13 1747324206.982752 sample_queries/q9_5a48.sql 4.765758037567139
16 14 1747324207.4961162 sample_queries/q31_2a39.sql 0.5133297443389893
16 15 1747324216.113958 sample_queries/q39_2a2781.sql 8.61780858039856
16 16 1747324221.714294 sample_queries/q8_6a505.sql 5.600303411483765
16 17 1747324227.1250668 sample_queries/q8_6a505.sql 5.410717248916626
16 18 1747324255.7192423 sample_queries/q22_8a27.sql 28.59414029121399
16 19 1747324261.766689 sample_queries/q40_2a8120.sql 6.0474114418029785
16 20 1747324263.3587835 sample_queries/q35_1a1508.sql 1.5920591354370117
16 21 1747324266.51308 sample_queries/q10_2a265.sql 3.1542603969573975
16 22 1747324268.1892126 sample_queries/q3_7a99.sql 1.676098108291626
16 23 1747324279.8541179 sample_queries/q5_8a423.sql 11.664868831634521
16 24 1747324288.0325959 sample_queries/q5_8a423.sql 8.178426265716553
Initial input channels: 10
Epoch 0 training loss: 4.454637051656328
Epoch 15 training loss: 0.45415426328264435
Epoch 30 training loss: 0.4710255239544244
Epoch 45 training loss: 0.39883286640818777
Epoch 60 training loss: 0.389869244961903
Epoch 75 training loss: 0.40943232769596166
Epoch 90 training loss: 0.3405931616394684
Epoch 105 training loss: 0.2910550733193241
Epoch 120 training loss: 0.33326761424541473
Epoch 135 training loss: 0.2914295782302988
Epoch 150 training loss: 0.2836077991102276
Epoch 165 training loss: 0.32461395474343463
Epoch 180 training loss: 0.34250857796648454
Epoch 195 training loss: 0.2633497382289377
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
17 0 1747324354.1453695 sample_queries/q22_8a27.sql 28.626373052597046
17 1 1747324364.2966065 sample_queries/q37_2a1291.sql 10.151198863983154
17 2 1747324381.7250369 sample_queries/q40_2a8120.sql 17.428390502929688
17 3 1747324384.3444438 sample_queries/q26_2a274.sql 2.6193735599517822
17 4 1747324384.962402 sample_queries/q33_2a156.sql 0.6179234981536865
17 5 1747324385.093621 sample_queries/q24_32a.sql 0.13118267059326172
17 6 1747324393.7181773 sample_queries/q6_16b.sql 8.624520063400269
17 7 1747324393.7756188 sample_queries/q29_6e.sql 0.05740523338317871
17 8 1747324403.811324 sample_queries/q19_2a471.sql 10.035673379898071
17 9 1747324406.3708 sample_queries/q26_2a274.sql 2.5594420433044434
17 10 1747324406.8879106 sample_queries/q31_2a39.sql 0.5170750617980957
17 11 1747324407.0195765 sample_queries/q24_32a.sql 0.13163065910339355
17 12 1747324435.8998513 sample_queries/q22_8a27.sql 28.880239963531494
17 13 1747324436.5227208 sample_queries/q33_2a156.sql 0.6228377819061279
17 14 1747324436.9767833 sample_queries/q21_2a396.sql 0.454026460647583
17 15 1747324439.225487 sample_queries/q28_13a.sql 2.2486705780029297
17 16 1747324448.1565838 sample_queries/q13_7a121.sql 8.931058406829834
17 17 1747324448.5810323 sample_queries/q21_2a396.sql 0.4244048595428467
17 18 1747324451.2834704 sample_queries/q14_6a349.sql 2.702406167984009
17 19 1747324453.4660513 sample_queries/q28_13a.sql 2.1825456619262695
17 20 1747324481.2293422 sample_queries/q22_8a27.sql 27.76325488090515
17 21 1747324481.361435 sample_queries/q24_32a.sql 0.13205742835998535
17 22 1747324486.2724512 sample_queries/q11_17e.sql 4.9109766483306885
17 23 1747324490.8492687 sample_queries/q9_5a48.sql 4.5767822265625
17 24 1747324494.7423024 sample_queries/q18_7a103.sql 3.8929991722106934
Initial input channels: 10
Epoch 0 training loss: 0.9516355271140734
Epoch 15 training loss: 0.40602216919263207
Epoch 30 training loss: 0.385330264767011
Epoch 45 training loss: 0.389206895728906
Epoch 60 training loss: 0.36616428891817726
Epoch 75 training loss: 0.3686268040910363
Epoch 90 training loss: 0.3674071083466212
Epoch 105 training loss: 0.34973777160048486
Epoch 120 training loss: 0.3693458157281081
Epoch 135 training loss: 0.32965228458245593
Epoch 150 training loss: 0.4417348726342122
Epoch 165 training loss: 0.4007307017842929
Epoch 180 training loss: 0.43660689890384674
Epoch 195 training loss: 0.3338207451005777
Stopped training after max epochs
Old model # regressions: 0 regression amount: 0
New model # regressions: 0 regression amount: 0
New model had no regressions.
18 0 1747324537.650783 sample_queries/q10_2a265.sql 3.22921085357666
18 1 1747324561.5377185 sample_queries/q31_2a39.sql 23.886901140213013
18 2 1747324563.4329045 sample_queries/q11_17e.sql 1.8951494693756104
18 3 1747324565.0397136 sample_queries/q30_18c.sql 1.6067709922790527
18 4 1747324573.6656473 sample_queries/q6_16b.sql 8.625898838043213
18 5 1747324578.6578412 sample_queries/q12_17a.sql 4.992151498794556
18 6 1747324579.6865482 sample_queries/q27_3c.sql 1.028672695159912
18 7 1747324585.7217696 sample_queries/q40_2a8120.sql 6.0351879596710205
18 8 1747324591.7219934 sample_queries/q19_2a471.sql 6.0001912117004395
18 9 1747324593.1824923 sample_queries/q15_18a.sql 1.4604644775390625
18 10 1747324594.7527382 sample_queries/q12_17a.sql 1.5702118873596191
18 11 1747324596.2630234 sample_queries/q35_1a1508.sql 1.5102510452270508
18 12 1747324601.9086318 sample_queries/q38_2a1870.sql 5.64557409286499
18 13 1747324604.4597137 sample_queries/q26_2a274.sql 2.5510449409484863
18 14 1747324606.6924698 sample_queries/q22_8a27.sql 2.232710838317871
18 15 1747324612.628802 sample_queries/q9_5a48.sql 5.9362969398498535
18 16 1747324618.5997832 sample_queries/q19_2a471.sql 5.970939636230469
18 17 1747324623.805763 sample_queries/q39_2a2781.sql 5.205944061279297
18 18 1747324626.3857014 sample_queries/q3_7a99.sql 2.5799062252044678
18 19 1747324627.415886 sample_queries/q27_3c.sql 1.030149221420288
18 20 1747324632.632123 sample_queries/q39_2a2781.sql 5.216202259063721
18 21 1747324634.900672 sample_queries/q22_8a27.sql 2.2685155868530273
18 22 1747324646.7022433 sample_queries/q6_16b.sql 11.801513433456421
18 23 1747324652.7315524 sample_queries/q37_2a1291.sql 6.029274225234985
18 24 1747324670.786225 sample_queries/q32_2a493.sql 18.054637670516968
